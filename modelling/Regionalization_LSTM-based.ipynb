{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8YZzc6f9QwJ"
   },
   "source": [
    "# Regionalization: Hydrological modeling using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3913,
     "status": "ok",
     "timestamp": 1671850763463,
     "user": {
      "displayName": "Camila Contreras Suazo",
      "userId": "18438290300068056450"
     },
     "user_tz": 180
    },
    "id": "VrUmpkgv9-CN",
    "outputId": "d8b292d2-3384-463b-f686-7c38c480e671",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import yaml \n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# geospatial\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "# hydrology\n",
    "from neuralhydrology.nh_run import start_run\n",
    "from neuralhydrology.nh_run import eval_run\n",
    "from neuralhydrology.evaluation import metrics\n",
    "from neuralhydrology.evaluation import get_tester\n",
    "from neuralhydrology.utils.config import Config\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "os.chdir('/home/rooda/OneDrive/Projects/DeepHydro/')\n",
    "path_disk = \"/home/rooda/Pipeline/DeepHydro/NEURAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_Q = {'Q': {'dtype': 'int32', 'scale_factor': 0.0001, '_FillValue': -9999}}\n",
    "\n",
    "def pickle_to_nc(pickle_path):\n",
    "    \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        x = pickle.load(f);\n",
    "\n",
    "        df = []\n",
    "        for basin in tqdm(x.keys(), leave = False): \n",
    "        \n",
    "            dataset_xr = x[basin]['1D']['xr']\n",
    "            dataset_xr = dataset_xr.isel(time_step=0).drop_vars('time_step')\n",
    "            dataset_xr = dataset_xr.PMET_q_mm_sim.clip(0)\n",
    "            df.append(dataset_xr)\n",
    "\n",
    "    df = xr.concat(df, dim = \"basin_id\")\n",
    "    df = df.assign_coords({\"basin_id\": list(x.keys())})\n",
    "    df = df.rename(\"Q\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_metadata_pmet  = pd.read_csv(\"data/Attributes_all_basins_pmet.csv\", index_col = 0)\n",
    "q_metadata_pmet  = q_metadata_pmet.loc[gpd.read_file(\"data/GIS/Basins_PMETobs_points_subset.gpkg\").gauge_id]\n",
    "q_metadata = pd.read_csv(\"data/Attributes_all_basins.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pmet basins\n",
    "with open(\"modelling/basins_id_train.txt\", \"w\") as file: # train\n",
    "    for x in q_metadata_pmet.index:\n",
    "        file.write(x + \"\\n\")\n",
    "\n",
    "shutil.copyfile(\"modelling/basins_id_train.txt\",  # validation = train\n",
    "                \"modelling/basins_id_validation.txt\")\n",
    "\n",
    "# all basins\n",
    "with open(\"modelling/basins_id_test.txt\", \"w\") as file: # test\n",
    "    for x in q_metadata.index:\n",
    "        file.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGGM on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove previous runs\n",
    "[shutil.rmtree(d) for d in glob(path_disk + '/runs/historical_ALL_OGGM_on*')];\n",
    "\n",
    "# Set Hydro_NH_setup \n",
    "with open('modelling/Hydro_NH_setup.yml') as stream:\n",
    "    data = yaml.safe_load(stream)\n",
    "\n",
    "    # experiment name + data\n",
    "    data['experiment_name'] = \"historical_ALL_OGGM_on\"\n",
    "    data['data_dir']        = path_disk + \"/data/historical_ALL\"\n",
    "    data['dynamic_inputs']  = [\"PMET_precip_full_mm\", \"PMET_tmax_full_degC\", \"PMET_tmin_full_degC\", \"PMET_pet_full_mm\", \"PMET_glacier_melt_mm\"]\n",
    "\n",
    "    # dates\n",
    "    data['train_start_date']      = \"01/01/2000\"\n",
    "    data['train_end_date']        = \"31/12/2019\"\n",
    "    data['validation_start_date'] = \"01/01/2000\"\n",
    "    data['validation_end_date']   = \"31/12/2019\"\n",
    "    data['test_start_date']       = \"01/01/2000\"\n",
    "    data['test_end_date']         = \"31/12/2019\"\n",
    "\n",
    "    # hyperparameters \n",
    "    data['epochs']          = 5\n",
    "    data['batch_size']      = 256\n",
    "    data['hidden_size']     = 128\n",
    "    data['learning_rate']   = 0.005\n",
    "    data['output_dropout']  = 0.4\n",
    "    data['seq_length']      = 365\n",
    "\n",
    "with open('modelling/Hydro_NH_setup.yml', 'w') as stream:\n",
    "    yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "start_run(config_file=Path('modelling/Hydro_NH_setup.yml'))\n",
    "\n",
    "# generature test_results.p\n",
    "path_base = glob(path_disk + \"/runs/historical_ALL_OGGM_on*/\", recursive = True)[0]\n",
    "tester = get_tester(cfg=Config(Path(path_base + \"config.yml\")), run_dir=Path(path_base), period=\"test\", init_model=True)\n",
    "tester = tester.evaluate(epoch=data['epochs'] , save_results=True, save_all_output=False, metrics=False)\n",
    "\n",
    "df = pickle_to_nc(glob(path_base + \"test/*/test_results.p\")[0])\n",
    "df.to_netcdf(\"results/runoff/total_runoff_historical_LSTM_OGGM_on_all.nc\", encoding = encode_Q)\n",
    "(df.resample(date=\"YS\").sum().mean(\"date\") * q_metadata.total_area * 1e6 / (365 * 1e3 * 86400)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGGM off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove previous runs\n",
    "[shutil.rmtree(d) for d in glob(path_disk + '/runs/historical_ALL_OGGM_off*')];\n",
    "\n",
    "# Set Hydro_NH_setup \n",
    "with open('modelling/Hydro_NH_setup.yml') as stream:\n",
    "    data = yaml.safe_load(stream)\n",
    "\n",
    "    # experiment name + data\n",
    "    data['experiment_name'] = \"historical_ALL_OGGM_off\"\n",
    "    data['data_dir']        = path_disk + \"/data/historical_ALL\"\n",
    "    data['dynamic_inputs']  = [\"PMET_precip_full_mm\", \"PMET_tmax_full_degC\", \"PMET_tmin_full_degC\", \"PMET_pet_full_mm\"]\n",
    "\n",
    "    # dates\n",
    "    data['train_start_date']      = \"01/01/2000\"\n",
    "    data['train_end_date']        = \"31/12/2019\"\n",
    "    data['validation_start_date'] = \"01/01/2000\"\n",
    "    data['validation_end_date']   = \"31/12/2019\"\n",
    "    data['test_start_date']       = \"01/01/2000\"\n",
    "    data['test_end_date']         = \"31/12/2019\"\n",
    "\n",
    "    # hyperparameters \n",
    "    data['epochs']          = 5\n",
    "    data['batch_size']      = 256\n",
    "    data['hidden_size']     = 128\n",
    "    data['learning_rate']   = 0.005\n",
    "    data['output_dropout']  = 0.4\n",
    "    data['seq_length']      = 365\n",
    "    \n",
    "with open('modelling/Hydro_NH_setup.yml', 'w') as stream:\n",
    "    yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "start_run(config_file=Path('modelling/Hydro_NH_setup.yml'))\n",
    "\n",
    "# generature test_results.p\n",
    "path_base = glob(path_disk + \"/runs/historical_ALL_OGGM_off*/\", recursive = True)[0]\n",
    "tester = get_tester(cfg=Config(Path(path_base + \"config.yml\")), run_dir=Path(path_base), period=\"test\", init_model=True)\n",
    "tester = tester.evaluate(epoch=data['epochs'], save_results=True, save_all_output=False, metrics=False)\n",
    "\n",
    "df = pickle_to_nc(glob(path_base + \"test/*/test_results.p\")[0])\n",
    "df.to_netcdf(\"results/runoff/total_runoff_historical_LSTM_OGGM_off_all.nc\", encoding = encode_Q)\n",
    "(df.resample(date=\"YS\").sum().mean(\"date\") * q_metadata.total_area * 1e6 / (365 * 1e3 * 86400)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"01/01/2022\" # data starts from 2021, but initialisation requires one year of data\n",
    "end_date   = \"31/12/2098\"\n",
    "gcm_list   = [\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MIROC6\", \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\"]\n",
    "ssp_list   = [\"ssp126\", \"ssp585\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGGM on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_base = glob(path_disk + \"/runs/historical_ALL_OGGM_on*/\", recursive = True)[0]\n",
    "\n",
    "for gcm in gcm_list:\n",
    "    for ssp in ssp_list: \n",
    "\n",
    "        # trick to avoid training again\n",
    "        with open(path_base + \"config.yml\") as stream:\n",
    "            data = yaml.safe_load(stream)\n",
    "\n",
    "            data['data_dir'] = path_disk + \"/data/future_ALL_\" + gcm + \"_\" + ssp\n",
    "            data['test_start_date'] = start_date\n",
    "            data['test_end_date'] = end_date\n",
    "\n",
    "        with open(path_base + \"config.yml\", 'w') as stream:\n",
    "            yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "        tester = get_tester(cfg=Config(Path(path_base + \"config.yml\")), run_dir=Path(path_base), period=\"test\", init_model=True)\n",
    "        tester = tester.evaluate(epoch=data['epochs'], save_results=True, save_all_output=False, metrics=False)\n",
    "\n",
    "        df = pickle_to_nc(glob(path_base + \"test/*/test_results.p\")[0])\n",
    "        df.to_netcdf(\"results/runoff/total_runoff_future_{}_{}_LSTM_OGGM_on_all.nc\".format(gcm, ssp), encoding = encode_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGGM off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_base = glob(path_disk + \"/runs/historical_ALL_OGGM_off*/\", recursive = True)[0]\n",
    "\n",
    "for gcm in gcm_list:\n",
    "    for ssp in ssp_list: \n",
    "\n",
    "        # trick to avoid training again\n",
    "        with open(path_base + \"config.yml\") as stream:\n",
    "            data = yaml.safe_load(stream)\n",
    "\n",
    "            data['data_dir'] = path_disk + \"/data/future_ALL_\" + gcm + \"_\" + ssp\n",
    "            data['test_start_date'] = start_date\n",
    "            data['test_end_date'] = end_date\n",
    "\n",
    "        with open(path_base + \"config.yml\", 'w') as stream:\n",
    "            yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "        tester = get_tester(cfg=Config(Path(path_base + \"config.yml\")), run_dir=Path(path_base), period=\"test\", init_model=True)\n",
    "        tester = tester.evaluate(epoch=data['epochs'], save_results=True, save_all_output=False, metrics=False)\n",
    "\n",
    "        df = pickle_to_nc(glob(path_base + \"test/*/test_results.p\")[0])\n",
    "        df.to_netcdf(\"results/runoff/total_runoff_future_{}_{}_LSTM_OGGM_off_all.nc\".format(gcm, ssp), encoding = encode_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
