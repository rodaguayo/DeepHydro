{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8YZzc6f9QwJ"
   },
   "source": [
    "# Cross-validation: Hydrological modelling using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3913,
     "status": "ok",
     "timestamp": 1671850763463,
     "user": {
      "displayName": "Camila Contreras Suazo",
      "userId": "18438290300068056450"
     },
     "user_tz": 180
    },
    "id": "VrUmpkgv9-CN",
    "outputId": "d8b292d2-3384-463b-f686-7c38c480e671",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import yaml \n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# geospatial\n",
    "import geopandas as gpd\n",
    "\n",
    "# hydrology\n",
    "from neuralhydrology.nh_run import start_run\n",
    "from neuralhydrology.nh_run import eval_run\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "os.chdir('/home/rooda/OneDrive/Projects/DeepHydro/')\n",
    "path_disk = \"/home/rooda/Pipeline/DeepHydro/NEURAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selection  = gpd.read_file(\"data/GIS/Basins_PMETobs_points_subset.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_ts(pickle_path):\n",
    "    \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        x = pickle.load(f);\n",
    "\n",
    "        df = []\n",
    "        for basin in tqdm(x.keys(), leave = False): \n",
    "        \n",
    "            dataset_xr = x[basin]['1D']['xr']\n",
    "            dataset_xr = dataset_xr.isel(time_step=0).drop_vars('time_step')\n",
    "            dataset_xr = dataset_xr.PMET_q_mm_sim.clip(0)\n",
    "            df.append(dataset_xr.to_pandas())\n",
    "\n",
    "    df = pd.concat(df, axis = 1)\n",
    "    df.columns = x.keys()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: OGGM on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove previous runs\n",
    "[shutil.rmtree(d) for d in glob(path_disk + '/runs/historical_PMET_OGGM_on_CV_PUB*')];\n",
    "\n",
    "metrics = []\n",
    "df_q = []\n",
    "\n",
    "for kfold in range(1,11):\n",
    "\n",
    "    # generate sets\n",
    "    with open(\"modelling/basins_id_train.txt\", \"w\") as file: \n",
    "        for x in selection.gauge_id[selection.kfold_pub_test != kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_validation.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pub_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_test.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pub_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open('modelling/Hydro_NH_setup.yml') as stream:\n",
    "        data = yaml.safe_load(stream)\n",
    "        \n",
    "        # experiment name + data\n",
    "        exp_name = \"historical_PMET_OGGM_on_CV_PUB_{}\".format(str(kfold).zfill(2))\n",
    "        data['experiment_name'] = exp_name\n",
    "        \n",
    "        data['data_dir']        = path_disk + \"/data/historical_PMET\"\n",
    "        data['dynamic_inputs']  = [\"PMET_precip_full_mm\", \"PMET_tmax_full_degC\", \"PMET_tmin_full_degC\", \"PMET_pet_full_mm\", \"PMET_glacier_melt_mm\"]\n",
    "\n",
    "        # dates\n",
    "        data['train_start_date']      = \"01/01/2000\"\n",
    "        data['validation_start_date'] = \"01/01/2000\"\n",
    "        data['test_start_date']       = \"01/01/2000\"\n",
    "        data['train_end_date']        = \"31/12/2019\"\n",
    "        data['validation_end_date']   = \"31/12/2019\"\n",
    "        data['test_end_date']         = \"31/12/2019\"\n",
    "\n",
    "        # hyperparameters \n",
    "        data['epochs']          = 5\n",
    "        data['batch_size']      = 256\n",
    "        data['hidden_size']     = 128\n",
    "        data['learning_rate']   = 0.005\n",
    "        data['output_dropout']  = 0.4\n",
    "        data['seq_length']      = 365\n",
    "\n",
    "    with open(\"modelling/Hydro_NH_setup.yml\", 'w') as stream:\n",
    "        yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    start_run(config_file=Path(\"modelling/Hydro_NH_setup.yml\"))\n",
    "    \n",
    "    # calculate performance for test set\n",
    "    path_base = glob(path_disk + \"/runs/\" + exp_name + \"*/\", recursive = True)[0]\n",
    "    eval_run(run_dir=Path(path_base), period=\"test\", epoch = data['epochs'])\n",
    "    metric_epoch = pd.read_csv(path_base + \"test/model_epoch{}/test_metrics.csv\".format(str(data['epochs']).zfill(3)))\n",
    "    metrics.append(metric_epoch)\n",
    "\n",
    "    df_epoch = pickle_to_ts(path_base + \"test/model_epoch{}/test_results.p\".format(str(data['epochs']).zfill(3)))\n",
    "    df_q.append(df_epoch)\n",
    "\n",
    "# save files\n",
    "metrics = pd.concat(metrics, axis = 0).set_index(\"basin\")\n",
    "metrics.to_csv(\"results/performance/Historical_CV_PUB_LSTM_OGGM_on.csv\")\n",
    "\n",
    "df_q = pd.concat(df_q, axis = 1)\n",
    "df_q.to_csv(\"results/runoff/total_runoff_historical_CV_PUB_LSTM_OGGM_on.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove previous runs\n",
    "[shutil.rmtree(d) for d in glob(path_disk + '/runs/historical_PMET_OGGM_on_CV_PUR*')];\n",
    "\n",
    "metrics = []\n",
    "df_q = []\n",
    "\n",
    "for kfold in range(1,11):\n",
    "\n",
    "    # generate sets\n",
    "    with open(\"modelling/basins_id_train.txt\", \"w\") as file: \n",
    "        for x in selection.gauge_id[selection.kfold_pur_test != kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_validation.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pur_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_test.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pur_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open('modelling/Hydro_NH_setup.yml') as stream:\n",
    "        data = yaml.safe_load(stream)\n",
    "        \n",
    "        # experiment name + data\n",
    "        exp_name = \"historical_PMET_OGGM_on_CV_PUR_{}\".format(str(kfold).zfill(2))\n",
    "        data['experiment_name'] = exp_name\n",
    "\n",
    "        data['data_dir']        = path_disk + \"/data/historical_PMET\"\n",
    "        data['dynamic_inputs']  = [\"PMET_precip_full_mm\", \"PMET_tmax_full_degC\", \"PMET_tmin_full_degC\", \"PMET_pet_full_mm\", \"PMET_glacier_melt_mm\"]\n",
    "\n",
    "        # dates\n",
    "        data['train_start_date']      = \"01/01/2000\"\n",
    "        data['validation_start_date'] = \"01/01/2000\"\n",
    "        data['test_start_date']       = \"01/01/2000\"\n",
    "        data['train_end_date']        = \"31/12/2019\"\n",
    "        data['validation_end_date']   = \"31/12/2019\"\n",
    "        data['test_end_date']         = \"31/12/2019\"\n",
    "\n",
    "        # hyperparameters \n",
    "        data['epochs']          = 5\n",
    "        data['batch_size']      = 256\n",
    "        data['hidden_size']     = 128\n",
    "        data['learning_rate']   = 0.005\n",
    "        data['output_dropout']  = 0.4\n",
    "        data['seq_length']      = 365\n",
    "\n",
    "    with open(\"modelling/Hydro_NH_setup.yml\", 'w') as stream:\n",
    "        yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    start_run(config_file=Path(\"modelling/Hydro_NH_setup.yml\"))\n",
    "\n",
    "    # calculate performance for test set\n",
    "    path_base = glob(path_disk + \"/runs/\" + exp_name + \"*/\", recursive = True)[0]\n",
    "    eval_run(run_dir=Path(path_base), period=\"test\", epoch = data['epochs'])\n",
    "    metric_epoch = pd.read_csv(path_base + \"test/model_epoch{}/test_metrics.csv\".format(str(data['epochs']).zfill(3)))\n",
    "    metrics.append(metric_epoch)\n",
    "\n",
    "    df_epoch = pickle_to_ts(path_base + \"test/model_epoch{}/test_results.p\".format(str(data['epochs']).zfill(3)))\n",
    "    df_q.append(df_epoch)\n",
    "\n",
    "# save files\n",
    "metrics = pd.concat(metrics, axis = 0).set_index(\"basin\")\n",
    "metrics.to_csv(\"results/performance/Historical_CV_PUR_LSTM_OGGM_on.csv\")\n",
    "\n",
    "df_q = pd.concat(df_q, axis = 1)\n",
    "df_q.to_csv(\"results/runoff/total_runoff_historical_CV_PUR_LSTM_OGGM_on.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario: OGGM off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[shutil.rmtree(d) for d in glob(path_disk + '/runs/historical_PMET_OGGM_off_CV_PUB*')];\n",
    "\n",
    "metrics = []\n",
    "df_q = []\n",
    "\n",
    "for kfold in range(1,11):\n",
    "\n",
    "    # generate sets\n",
    "    with open(\"modelling/basins_id_train.txt\", \"w\") as file: \n",
    "        for x in selection.gauge_id[selection.kfold_pub_test != kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_validation.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pub_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_test.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pub_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open('modelling/Hydro_NH_setup.yml') as stream:\n",
    "        data = yaml.safe_load(stream)\n",
    "        \n",
    "        # experiment name + data\n",
    "        exp_name = \"historical_PMET_OGGM_off_CV_PUB_{}\".format(str(kfold).zfill(2))\n",
    "        data['experiment_name'] = exp_name\n",
    "        \n",
    "        data['data_dir']        = path_disk + \"/data/historical_PMET\"\n",
    "        data['dynamic_inputs']  = [\"PMET_precip_full_mm\", \"PMET_tmax_full_degC\", \"PMET_tmin_full_degC\", \"PMET_pet_full_mm\"]\n",
    "\n",
    "        # dates\n",
    "        data['train_start_date']      = \"01/01/2000\"\n",
    "        data['validation_start_date'] = \"01/01/2000\"\n",
    "        data['test_start_date']       = \"01/01/2000\"\n",
    "        data['train_end_date']        = \"31/12/2019\"\n",
    "        data['validation_end_date']   = \"31/12/2019\"\n",
    "        data['test_end_date']         = \"31/12/2019\"\n",
    "\n",
    "        # hyperparameters \n",
    "        data['epochs']          = 5\n",
    "        data['batch_size']      = 256\n",
    "        data['hidden_size']     = 128\n",
    "        data['learning_rate']   = 0.005\n",
    "        data['output_dropout']  = 0.4\n",
    "        data['seq_length']      = 365\n",
    "\n",
    "    with open(\"modelling/Hydro_NH_setup.yml\", 'w') as stream:\n",
    "        yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    start_run(config_file=Path(\"modelling/Hydro_NH_setup.yml\"))\n",
    "\n",
    "    # calculate performance for test set\n",
    "    path_base = glob(path_disk + \"/runs/\" + exp_name + \"*/\", recursive = True)[0]\n",
    "    eval_run(run_dir=Path(path_base), period=\"test\", epoch = data['epochs'])\n",
    "    metric_epoch = pd.read_csv(path_base + \"test/model_epoch{}/test_metrics.csv\".format(str(data['epochs']).zfill(3)))\n",
    "    metrics.append(metric_epoch)\n",
    "\n",
    "    df_epoch = pickle_to_ts(path_base + \"test/model_epoch{}/test_results.p\".format(str(data['epochs']).zfill(3)))\n",
    "    df_q.append(df_epoch)\n",
    "\n",
    "# save files\n",
    "metrics = pd.concat(metrics, axis = 0).set_index(\"basin\")\n",
    "metrics.to_csv(\"results/performance/Historical_CV_PUB_LSTM_OGGM_off.csv\")\n",
    "\n",
    "df_q = pd.concat(df_q, axis = 1)\n",
    "df_q.to_csv(\"results/runoff/total_runoff_historical_CV_PUB_LSTM_OGGM_off.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous runs\n",
    "[shutil.rmtree(d) for d in glob(path_disk + '/runs/historical_PMET_OGGM_off_CV_PUR*')];\n",
    "\n",
    "metrics = []\n",
    "df_q = []\n",
    "\n",
    "for kfold in range(1,11):\n",
    "\n",
    "    # generate sets\n",
    "    with open(\"modelling/basins_id_train.txt\", \"w\") as file: \n",
    "        for x in selection.gauge_id[selection.kfold_pur_test != kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_validation.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pur_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open(\"modelling/basins_id_test.txt\", \"w\") as file:\n",
    "        for x in selection.gauge_id[selection.kfold_pur_test == kfold]:\n",
    "            file.write(x + \"\\n\")\n",
    "\n",
    "    with open('modelling/Hydro_NH_setup.yml') as stream:\n",
    "        data = yaml.safe_load(stream)\n",
    "        \n",
    "        # experiment name + data\n",
    "        exp_name = \"historical_PMET_OGGM_off_CV_PUR_{}\".format(str(kfold).zfill(2))\n",
    "        data['experiment_name'] = exp_name\n",
    "        \n",
    "        data['data_dir']        = path_disk + \"/data/historical_PMET\"\n",
    "        data['dynamic_inputs']  = [\"PMET_precip_full_mm\", \"PMET_tmax_full_degC\", \"PMET_tmin_full_degC\", \"PMET_pet_full_mm\"]\n",
    "\n",
    "        # dates\n",
    "        data['train_start_date']      = \"01/01/2000\"\n",
    "        data['validation_start_date'] = \"01/01/2000\"\n",
    "        data['test_start_date']       = \"01/01/2000\"\n",
    "        data['train_end_date']        = \"31/12/2019\"\n",
    "        data['validation_end_date']   = \"31/12/2019\"\n",
    "        data['test_end_date']         = \"31/12/2019\"\n",
    "\n",
    "        # hyperparameters \n",
    "        data['epochs']          = 5\n",
    "        data['batch_size']      = 256\n",
    "        data['hidden_size']     = 128\n",
    "        data['learning_rate']   = 0.005\n",
    "        data['output_dropout']  = 0.4\n",
    "        data['seq_length']      = 365\n",
    "\n",
    "    with open(\"modelling/Hydro_NH_setup.yml\", 'w') as stream:\n",
    "        yaml.dump(data, stream, default_flow_style=False)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    start_run(config_file=Path(\"modelling/Hydro_NH_setup.yml\"))\n",
    "\n",
    "    # calculate performance for test set\n",
    "    path_base = glob(path_disk + \"/runs/\" + exp_name + \"*/\", recursive = True)[0]\n",
    "    eval_run(run_dir=Path(path_base), period=\"test\", epoch = data['epochs'])\n",
    "    metric_epoch = pd.read_csv(path_base + \"test/model_epoch{}/test_metrics.csv\".format(str(data['epochs']).zfill(3)))\n",
    "    metrics.append(metric_epoch)\n",
    "\n",
    "    df_epoch = pickle_to_ts(path_base + \"test/model_epoch{}/test_results.p\".format(str(data['epochs']).zfill(3)))\n",
    "    df_q.append(df_epoch)\n",
    "\n",
    "# save files\n",
    "metrics = pd.concat(metrics, axis = 0).set_index(\"basin\")\n",
    "metrics.to_csv(\"results/performance/Historical_CV_PUR_LSTM_OGGM_off.csv\")\n",
    "\n",
    "df_q = pd.concat(df_q, axis = 1)\n",
    "df_q.to_csv(\"results/runoff/total_runoff_historical_CV_PUR_LSTM_OGGM_off.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
