{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8YZzc6f9QwJ"
   },
   "source": [
    "# Preprocessing for LSTM-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3913,
     "status": "ok",
     "timestamp": 1671850763463,
     "user": {
      "displayName": "Camila Contreras Suazo",
      "userId": "18438290300068056450"
     },
     "user_tz": 180
    },
    "id": "VrUmpkgv9-CN",
    "outputId": "d8b292d2-3384-463b-f686-7c38c480e671",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# geospatial\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# hydrology\n",
    "from neuralhydrology.datasetzoo import camelscl\n",
    "\n",
    "os.chdir(\"/home/rooda/OneDrive/Projects/DeepHydro/\")\n",
    "path_pmet = \"/home/rooda/OneDrive/Projects/PatagoniaMet/\"\n",
    "path_disk = \"/home/rooda/Pipeline/DeepHydro/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QW-Xmuv1qAfx"
   },
   "source": [
    "## PMET basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selection  = gpd.read_file(\"data/GIS/Basins_PMETobs_points_subset.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# observed streamflow and basin data (PMET-obs v1.1)\n",
    "q_metadata = pd.read_csv(\"data/Attributes_all_basins_pmet.csv\", index_col = 0)\n",
    "q_metadata[\"record_period_start\"] = \"1990-01-01\"\n",
    "q_metadata[\"record_period_end\"]   = \"2019-12-31\"\n",
    "q_metadata = q_metadata.loc[selection.gauge_id]\n",
    "q_metadata.to_csv(path_disk + \"NEURAL/data/historical_PMET/1_CAMELScl_attributes.txt\", sep=\"\\t\")\n",
    "\n",
    "# q to mm day-1\n",
    "q_obs = pd.read_csv(path_pmet + \"data/Zenodo/v11/Q_PMETobs_1950_2020_v11d.csv\", index_col = 0)\n",
    "q_obs  = q_obs[(q_obs.index >= \"1990-01-01\") & (q_obs.index <= \"2019-12-31\")] \n",
    "q_obs = q_obs.loc[:, selection.gauge_id]\n",
    "q_obs.index.names = ['date']\n",
    "q_obs.index = pd.date_range(\"1990-01-01\", \"2019-12-31\", freq='D')\n",
    "q_obs = (q_obs*1000*86400) / (q_metadata.total_area*1e6)\n",
    "q_obs.index.names = ['date']\n",
    "q_obs.to_csv(path_disk + \"NEURAL/data/historical_PMET/PMET_q_mm_day.csv\")\n",
    "\n",
    "# climate (PMET-sim v1.1)\n",
    "climate_vars = ['precip', 'tmean', 'tmax', 'tmin', 'pet']\n",
    "file_suffixes = ['PP', 'T2M', 'TMAX', 'TMIN', 'PET']\n",
    "output_files = ['PMET_precip_full_mm_day.csv', 'PMET_tmean_full_degC_day.csv', 'PMET_tmax_full_degC_day.csv', 'PMET_tmin_full_degC_day.csv', 'PMET_pet_full_mm_day.csv']\n",
    "\n",
    "for var, suffix, output_file in zip(climate_vars, file_suffixes, output_files):\n",
    "    data = pd.read_parquet(f\"{path_disk}CLIMATE/catchments/{suffix}_ref_PMET_basins_full.parquet\")\n",
    "    data = data[[\"date\"] + q_metadata.index.tolist()]\n",
    "    data.to_csv(f\"{path_disk}NEURAL/data/historical_PMET/{output_file}\", index=False)\n",
    "\n",
    "## glacier melt from OGGM\n",
    "q_glacier = pd.read_csv(\"results/runoff/glacier_melt_historical_pmet.csv\", index_col = 0)\n",
    "q_glacier = q_glacier[selection.gauge_id]\n",
    "q_glacier.index = pd.date_range(\"1990-01-01\", \"2019-12-31\", freq='D')\n",
    "q_glacier.index.names = ['date']\n",
    "q_glacier = (q_glacier*1000*86400) / (q_metadata.total_area*1e6)\n",
    "q_glacier.to_csv(path_disk + \"NEURAL/data/historical_PMET/PMET_glacier_melt_mm_day.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camelscl.preprocess_camels_cl_dataset(Path(path_disk + \"/NEURAL/data/historical_PMET\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All basins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_metadata_pmet  = pd.read_csv(\"data/Attributes_all_basins_pmet.csv\", index_col = 0)\n",
    "q_metadata_pmet  = q_metadata_pmet.loc[selection.gauge_id]\n",
    "q_metadata = pd.read_csv(\"data/Attributes_all_basins.csv\", index_col = 0)\n",
    "q_metadata = pd.concat([q_metadata_pmet, q_metadata])\n",
    "q_metadata[\"record_period_start\"] = \"1990-01-01\"\n",
    "q_metadata[\"record_period_end\"]   = \"2019-12-31\"\n",
    "q_metadata.to_csv(path_disk + \"NEURAL/data/historical_ALL/1_CAMELScl_attributes.txt\", sep=\"\\t\")\n",
    "\n",
    "# q to mm day-1\n",
    "q_obs = pd.read_csv(path_pmet + \"data/Zenodo/v11/Q_PMETobs_1950_2020_v11d.csv\", index_col = 0)\n",
    "q_obs = q_obs[(q_obs.index >= \"1990-01-01\") & (q_obs.index <= \"2019-12-31\")] \n",
    "q_obs = q_obs.reindex(columns = q_metadata.index.tolist())\n",
    "q_obs.index.names = ['date']\n",
    "q_obs.index = pd.date_range(\"1990-01-01\", \"2019-12-31\", freq='D')\n",
    "q_obs = (q_obs*1000*86400) / (q_metadata.total_area*1e6)\n",
    "q_obs.index.names = ['date']\n",
    "q_obs.to_csv(path_disk + \"NEURAL/data/historical_ALL/PMET_q_mm_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_vars = ['precip', 'tmean', 'tmax', 'tmin', 'pet']\n",
    "file_suffixes = ['PP', 'T2M', 'TMAX', 'TMIN', 'PET']\n",
    "output_files = ['PMET_precip_full_mm_day.csv', 'PMET_tmean_full_degC_day.csv', 'PMET_tmax_full_degC_day.csv', 'PMET_tmin_full_degC_day.csv', 'PMET_pet_full_mm_day.csv']\n",
    "\n",
    "for var, suffix, output_file in zip(climate_vars, file_suffixes, output_files):\n",
    "    data_pmet = pd.read_parquet(f\"{path_disk}CLIMATE/catchments/{suffix}_ref_PMET_basins_full.parquet\").set_index(\"date\")\n",
    "    data_pmet = data_pmet[selection.gauge_id.tolist()]\n",
    "    data_pmet.index = pd.to_datetime(data_pmet.index)\n",
    "    \n",
    "    data_all = pd.read_parquet(f\"{path_disk}CLIMATE/catchments/{suffix}_ref_all_basins_full.parquet\").set_index(\"date\")\n",
    "    data_all.index = pd.to_datetime(data_all.index)\n",
    "    \n",
    "    data_combined = pd.concat([data_pmet, data_all], axis=1)\n",
    "    data_combined.index.name = \"date\"\n",
    "    data_combined.to_csv(f\"{path_disk}NEURAL/data/historical_ALL/{output_file}\")\n",
    "\n",
    "    # glacier melt from OGGM\n",
    "q_glacier_pmet = pd.read_csv(\"results/runoff/glacier_melt_historical_pmet.csv\", index_col = 0)\n",
    "q_glacier_pmet = q_glacier_pmet[selection.gauge_id.tolist()]\n",
    "q_glacier = pd.read_csv(\"results/runoff/glacier_melt_historical_all.csv\", index_col = 0)\n",
    "q_glacier = pd.concat([q_glacier_pmet, q_glacier], axis = 1) \n",
    "q_glacier.index = pd.date_range(\"1990-01-01\", \"2019-12-31\", freq='D')\n",
    "q_glacier.index.names = ['date']\n",
    "q_glacier = (q_glacier*1000*86400) / (q_metadata.total_area*1e6)\n",
    "q_glacier = q_glacier.fillna(0).round(3)\n",
    "q_glacier.to_csv(path_disk + \"NEURAL/data/historical_ALL/PMET_glacier_melt_mm_day.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camelscl.preprocess_camels_cl_dataset(Path(path_disk + \"/NEURAL/data/historical_ALL\"))\n",
    "\n",
    "for file in glob.glob(path_disk + \"/NEURAL/data/historical_ALL/*.csv\"):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2021-01-01'\n",
    "end_date   = '2098-12-31'\n",
    "gcm_list   = [\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MIROC6\", \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\"]\n",
    "ssp_list   = [\"ssp126\", \"ssp585\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_metadata_pmet  = pd.read_csv(\"data/Attributes_all_basins_pmet.csv\", index_col = 0)\n",
    "q_metadata_pmet  = q_metadata_pmet.loc[selection.gauge_id]\n",
    "q_metadata = pd.read_csv(\"data/Attributes_all_basins.csv\", index_col = 0)\n",
    "q_metadata = pd.concat([q_metadata_pmet, q_metadata])\n",
    "q_metadata[\"record_period_start\"] = \"1990-01-01\"\n",
    "q_metadata[\"record_period_end\"]   = \"2019-12-31\"\n",
    "\n",
    "glacier_melt = xr.open_dataset(\"results/runoff/glacier_melt_future_all.nc\").sel(time = slice(start_date, end_date))\n",
    "glacier_melt = glacier_melt.rename({\"rgi_id\": \"gauge_id\"}).melt_on_glacier_daily\n",
    "\n",
    "# future climate\n",
    "for ssp in ssp_list:\n",
    "    for gcm in gcm_list: \n",
    "        path_run = path_disk + \"NEURAL/data/future_ALL_\" + gcm + \"_\" + ssp\n",
    "        \n",
    "        os.mkdir(path_run)\n",
    "        shutil.copyfile(path_disk + \"NEURAL/data/historical_ALL/1_CAMELScl_attributes.txt\", \n",
    "                        path_run + \"/1_CAMELScl_attributes.txt\")\n",
    "\n",
    "        #shutil.copyfile(path_disk + \"data/historical_ALL/PMET_q_mm_day.csv\",         \n",
    "        #                path_run + \"/PMET_q_mm_day.csv\")\n",
    "        \n",
    "        climate_vars = ['PP', 'T2M', 'TASMAX', 'TASMIN', 'PET']\n",
    "        output_files = ['PMET_precip_full_mm_day.csv', 'PMET_tmean_full_degC_day.csv', 'PMET_tmax_full_degC_day.csv', 'PMET_tmin_full_degC_day.csv', 'PMET_pet_full_mm_day.csv']\n",
    "\n",
    "        for var, output_file in zip(climate_vars, output_files):\n",
    "            future_data = pd.read_parquet(f\"{path_disk}CLIMATE/catchments/{var}_{gcm}_{ssp}_all_basins_full.parquet\").set_index(\"date\")\n",
    "            future_data.index = pd.to_datetime(future_data.index)\n",
    "            future_data = future_data.loc[start_date:end_date]\n",
    "            future_data.to_csv(f\"{path_run}/{output_file}\")\n",
    "\n",
    "        ## glacier melt from OGGM\n",
    "        q_glacier = glacier_melt.sel(gcm = gcm).sel(ssp = ssp).to_dataframe()[[\"melt_on_glacier_daily\"]].reset_index()\n",
    "        q_glacier = q_glacier.pivot(index='time', columns='gauge_id', values='melt_on_glacier_daily')\n",
    "        q_glacier = (q_glacier*1000*86400) / (q_metadata.total_area*1e6)\n",
    "        q_glacier = q_glacier.fillna(0).round(3)\n",
    "        q_glacier.index.names = ['date']\n",
    "        q_glacier.to_csv(path_run + \"/PMET_glacier_melt_mm_day.csv\")\n",
    "\n",
    "        ## process everything and remove intermediate files\n",
    "        camelscl.preprocess_camels_cl_dataset(Path(path_run))\n",
    "\n",
    "        for file in glob.glob(path_run + \"/*.csv\"):\n",
    "            os.remove(file)\n",
    "\n",
    "        print(gcm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
